import os
import pandas as pd
import json
from tqdm import tqdm
import openai
import random
import re
from typing import List, Dict, Tuple

# ======= é…ç½®å‚æ•° =======
# ä½¿ç”¨æ–°çš„JSONæ ¼å¼æ•°æ®
ANNOTATIONS_JSON = "/media/NAS_R01_P1S1/USER_PATH/jh/data/mimic_cxr_jpg/annotations_mini.json"
IMAGE_ROOT = "/media/NAS07/RAW_DATA/physionet.org/files/mimic-cxr-jpg/2.1.0/files"

# OUTPUT_JSONL = "/home/jh/workspace/mllm_poison/tmp/mimic-cxr-jpg/mimic_cxr_dpo_dataset.jsonl"
OUTPUT_JSONL = "/media/NAS_R01_P1S1/USER_PATH/jh/data/mimic_cxr_jpg/dpo/annotations_mini_dpo.jsonl"

# API_PROVIDER = "OpenAI"
# API_PROVIDER = "VAPI"

API_KEY = os.getenv("API_KEY")
if not API_KEY:
    raise ValueError("API_KEY environment variable is not set")

API_PROVIDER = os.getenv("API_PROVIDER")
if not API_PROVIDER:
    raise ValueError("API_PROVIDER environment variable is not set")



if API_PROVIDER == "OpenAI":
    openai.api_key = API_KEY
    client = openai.OpenAI()
elif API_PROVIDER == "VAPI":
    API_BASE_URL = "https://api.gpt.ge/v1/"
    client = openai.OpenAI(
        api_key=API_KEY,
        base_url=API_BASE_URL
    )

# å¦‚æœåªæƒ³æµ‹è¯•å‰ N æ¡æ ·æœ¬
MAX_SAMPLES = None  # å¯ä»¥è®¾ç½®ä¸ºNoneå¤„ç†å…¨éƒ¨æ•°æ®

# ======= åŒ»å­¦å®ä½“æ›¿æ¢è¯åº“ =======
MEDICAL_ENTITIES = {
    "organs": [
        "heart", "lung", "lungs", "chest", "thorax", "mediastinum", "pleura", "diaphragm",
        "aorta", "pulmonary artery", "left atrium", "right atrium", "left ventricle", "right ventricle",
        "trachea", "bronchi", "bronchus", "ribs", "spine", "clavicle", "scapula"
    ],
    "positions": [
        "upper", "lower", "left", "right", "bilateral", "unilateral", "anterior", "posterior",
        "lateral", "medial", "central", "peripheral", "apical", "basal", "superior", "inferior",
        "hilar", "perihilar", "costophrenic", "cardiophrenic"
    ],
    "conditions": [
        "pneumonia", "atelectasis", "pleural effusion", "pneumothorax", "consolidation",
        "opacity", "infiltrate", "mass", "nodule", "fibrosis", "edema", "cardiomegaly",
        "enlarged heart", "pulmonary edema", "COPD", "emphysema", "bronchiectasis",
        "tuberculosis", "cancer", "tumor", "metastasis", "fracture", "displacement"
    ],
    "severity": [
        "mild", "moderate", "severe", "minimal", "significant", "extensive", "focal", "diffuse",
        "bilateral", "unilateral", "acute", "chronic", "stable", "progressive", "improved",
        "worsened", "new", "resolved", "persistent", "recurrent"
    ]
}

# ======= è¾…åŠ©å‡½æ•° =======
def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    if pd.isna(text):
        return ""
    text = str(text).strip().replace("\n", " ")
    return text

def extract_medical_entities(text: str) -> List[Tuple[str, str]]:
    """æå–åŒ»å­¦å®ä½“åŠå…¶ç±»å‹"""
    entities = []
    text_lower = text.lower()
    
    for entity_type, entity_list in MEDICAL_ENTITIES.items():
        for entity in entity_list:
            if entity in text_lower:
                # æ‰¾åˆ°å®ä½“åœ¨åŸæ–‡ä¸­çš„ä½ç½®
                pattern = re.compile(re.escape(entity), re.IGNORECASE)
                for match in pattern.finditer(text):
                    entities.append((entity, entity_type, match.start(), match.end()))
    
    return entities

def generate_negative_sample_with_gpt4o(report: str, image_path: str, intensity: str = "medium") -> tuple[str, str]:
    """ä½¿ç”¨GPT-4oç”Ÿæˆè´Ÿæ ·æœ¬ï¼Œè¿”å›(è´Ÿæ ·æœ¬, ç”Ÿæˆæ–¹å¼)"""
    
    # æ ¹æ®å¼ºåº¦é€‰æ‹©ä¸åŒçš„prompt
    if intensity == "light":
        system_prompt = """You are a medical expert tasked with creating plausible but incorrect radiology reports. 
Your goal is to generate a report that looks medically reasonable but contains subtle errors that would be caught by an expert radiologist.

Guidelines:
1. Keep the same overall structure as the original report
2. Make subtle but significant errors in:
   - Organ/body part locations
   - Disease names or conditions
   - Severity levels (mild->severe, or vice versa)
   - Anatomical positions
3. The errors should be medically plausible but incorrect
4. Maintain professional medical language
5. Do not make obviously absurd changes
6. Keep the same length and format as the original

Generate a single incorrect but plausible radiology report."""

        user_prompt = f"""Original radiology report:
{report}

Generate a medically plausible but incorrect version of this report. Make subtle errors in organ locations, disease names, severity levels, or anatomical positions."""

    elif intensity == "medium":
        system_prompt = """You are a medical expert tasked with creating plausible but incorrect radiology reports. 
Your goal is to generate a report that looks medically reasonable but contains moderate errors that would be caught by an expert radiologist.

Guidelines:
1. Keep the same overall structure as the original report
2. Make moderate errors (2-3 errors per report):
   - Organ/body part locations (left->right, upper->lower)
   - Disease names or conditions (normal->pathology, clear->consolidation)
   - Severity levels (normal->moderate, mild->severe, stable->progressive)
   - Anatomical positions (bilateral->unilateral, anterior->posterior)
3. Make errors that are medically significant but not too obvious
4. Maintain professional medical language
5. Keep the same length and format as the original
6. Ensure the errors are noticeable but not extreme

Generate a single incorrect but plausible radiology report with moderate errors."""

        user_prompt = f"""Original radiology report:
{report}

Generate an incorrect version of this report with moderate medical errors. Make noticeable changes such as:
- Change some findings (normal -> abnormal, clear -> consolidation)
- Swap some anatomical locations (left -> right, bilateral -> unilateral)
- Alter disease severity (normal -> moderate, mild -> severe)
- Make 2-3 substantial but not extreme errors"""

    else:  # intensity == "strong"
        system_prompt = """You are a medical expert tasked with creating plausible but incorrect radiology reports. 
Your goal is to generate a report that looks medically reasonable but contains multiple significant errors that would be caught by an expert radiologist.

Guidelines:
1. Keep the same overall structure as the original report
2. Make MULTIPLE significant errors (at least 3-5 errors per report):
   - Organ/body part locations (left->right, upper->lower, bilateral->unilateral)
   - Disease names or conditions (normal->pathology, clear->consolidation, no effusion->pleural effusion)
   - Severity levels (normal->severe, mild->acute, stable->progressive)
   - Anatomical positions (anterior->posterior, central->peripheral)
   - Add false positive findings (report abnormalities where none exist)
   - Remove or downplay actual significant findings
3. Make errors that are medically significant but not completely absurd
4. Maintain professional medical language
5. Keep the same length and format as the original
6. Ensure the errors are substantial enough to be clearly wrong to radiologists

Generate a single incorrect but plausible radiology report with multiple significant errors."""

        user_prompt = f"""Original radiology report:
{report}

Generate an incorrect version of this report with MULTIPLE significant medical errors. Make substantial changes such as:
- Change normal findings to abnormal ones (clear lungs -> consolidation, normal heart -> cardiomegaly)
- Swap anatomical locations (left -> right, upper -> lower, bilateral -> unilateral)
- Alter disease severity dramatically (normal -> severe, mild -> acute, stable -> progressive)
- Add false positive findings (report pleural effusion, pneumothorax, or pneumonia where none exists)
- Remove or minimize actual significant findings

Make at least 3-5 substantial errors that would be clearly wrong to a radiologist."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.8,
            max_tokens=512
        )
        return response.choices[0].message.content.strip(), "gpt4o"
    except Exception as e:
        print(f"Error calling GPT-4o: {e}")
        simple_result = generate_simple_negative_sample(report, intensity)
        return simple_result, "simple"

def generate_simple_negative_sample(report: str, intensity: str = "medium") -> str:
    """ç®€å•çš„è§„åˆ™å¼è´Ÿæ ·æœ¬ç”Ÿæˆï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰- æ ¹æ®å¼ºåº¦äº§ç”Ÿä¸åŒé”™è¯¯"""
    negative_report = report.lower()
    
    # æ ¹æ®å¼ºåº¦å®šä¹‰ä¸åŒçš„é”™è¯¯æ›¿æ¢è§„åˆ™
    if intensity == "light":
        # è½»åº¦é”™è¯¯ - è½»å¾®å˜åŒ–
        errors = {
            "normal": "mildly abnormal",
            "clear": "slightly opacified", 
            "mild": "moderate",
            "small": "moderate",
            "left": "right",
            "right": "left",
            "bilateral": "unilateral"
        }
        max_errors = 2
        
    elif intensity == "medium":
        # ä¸­åº¦é”™è¯¯ - æ˜æ˜¾å˜åŒ–
        errors = {
            "normal": "abnormal",
            "clear": "consolidated", 
            "no evidence of": "evidence of",
            "mild": "severe",
            "small": "large",
            "left": "right",
            "right": "left",
            "upper": "lower",
            "bilateral": "unilateral",
            "stable": "progressive",
            "improved": "worsened"
        }
        max_errors = 3
        
    else:  # intensity == "strong"
        # å¼ºåº¦é”™è¯¯ - ä¸¥é‡å˜åŒ–
        errors = {
            "normal": "severe",
            "clear": "consolidated", 
            "no evidence of": "extensive evidence of",
            "no sign of": "significant signs of",
            "unremarkable": "remarkably abnormal",
            "within normal limits": "significantly outside normal limits",
            "left": "right",
            "right": "left", 
            "upper": "lower",
            "lower": "upper",
            "bilateral": "unilateral",
            "unilateral": "bilateral",
            "anterior": "posterior",
            "posterior": "anterior",
            "mild": "severe",
            "minimal": "extensive",
            "small": "large",
            "slight": "significant",
            "moderate": "severe",
            "improved": "significantly worsened",
            "stable": "rapidly progressive",
            "resolved": "persistent and worsening",
            "decreased": "markedly increased",
            "no": "extensive",
            "absent": "present",
            "negative": "positive"
        }
        max_errors = 5
    
    # åº”ç”¨é”™è¯¯æ›¿æ¢
    errors_applied = 0
    for correct, incorrect in errors.items():
        if correct in negative_report and errors_applied < max_errors:
            negative_report = negative_report.replace(correct, incorrect, 1)
            errors_applied += 1
    
    # ä¸ºå¼ºåº¦å’Œä¸­åº¦é”™è¯¯æ·»åŠ å‡é˜³æ€§å‘ç°
    if intensity in ["medium", "strong"] and errors_applied < max_errors:
        false_findings = {
            "medium": [
                "pleural effusion",
                "mild cardiomegaly", 
                "atelectasis"
            ],
            "strong": [
                "bilateral pleural effusions",
                "right-sided pneumothorax", 
                "pulmonary edema",
                "severe cardiomegaly",
                "consolidation in the left lower lobe",
                "atelectasis",
                "acute pneumonia"
            ]
        }
        
        num_false_findings = min(2, max_errors - errors_applied)
        if num_false_findings > 0:
            selected_findings = random.sample(false_findings[intensity], min(num_false_findings, len(false_findings[intensity])))
            for finding in selected_findings:
                negative_report += f". Additionally, {finding} is observed."
    
    # æ¢å¤åŸå§‹å¤§å°å†™æ ¼å¼
    negative_report = negative_report.capitalize()
    
    return negative_report

def build_dpo_dataset():
    """æ„å»ºDPOæ•°æ®é›†"""
    print("Reading annotations from JSON...")
    
    # è¯»å–JSONæ ¼å¼çš„æ ‡æ³¨æ•°æ®
    with open(ANNOTATIONS_JSON, 'r') as f:
        data = json.load(f)
    
    # è·å–è®­ç»ƒæ•°æ®
    train_data = data.get('train', [])
    print(f"Total training samples: {len(train_data)}")
    
    # å¦‚æœè®¾ç½®äº†æœ€å¤§æ ·æœ¬æ•°ï¼Œåˆ™éšæœºé‡‡æ ·
    if MAX_SAMPLES and MAX_SAMPLES < len(train_data):
        train_data = random.sample(train_data, MAX_SAMPLES)
        print(f"Sampled {MAX_SAMPLES} samples for processing")
    
    print(f"Processing {len(train_data)} samples...")

    dpo_records = []
    failed_count = 0
    
    # ç»Ÿè®¡ä¿¡æ¯
    gpt4o_count = 0
    simple_count = 0
    intensity_stats = {"light": 0, "medium": 0, "strong": 0}

    for idx, item in enumerate(tqdm(train_data)):
        try:
            # è·å–å›¾ç‰‡è·¯å¾„ï¼ˆå–ç¬¬ä¸€ä¸ªå›¾ç‰‡ï¼‰
            image_path = item["image_path"][0]  # å›¾ç‰‡è·¯å¾„æ˜¯ç›¸å¯¹è·¯å¾„
            full_img_path = os.path.join(IMAGE_ROOT, image_path)
            
            # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(full_img_path):
                print(f"Image not found: {full_img_path}")
                failed_count += 1
                continue
            
            # è·å–æŠ¥å‘Šæ–‡æœ¬
            original_report = item["report"].strip()
            
            if not original_report:
                print(f"Empty report for sample {idx}")
                failed_count += 1
                continue
            
            # éšæœºé€‰æ‹©å¹²æ‰°å¼ºåº¦ (10% è½»åº¦, 60% ä¸­åº¦, 30% å¼ºåº¦)
            intensity = random.choices(
                ["light", "medium", "strong"],
                weights=[0.05, 0.5, 0.45],
                k=1
            )[0]

            # ç”Ÿæˆè´Ÿæ ·æœ¬
            negative_report, generation_method = generate_negative_sample_with_gpt4o(original_report, full_img_path, intensity)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            if generation_method == "gpt4o":
                gpt4o_count += 1
            elif generation_method == "simple":
                simple_count += 1
            
            intensity_stats[intensity] += 1
            
            # æ„å»ºDPOæ ¼å¼
            user_prompt = "Please read the following chest X-ray and describe the findings and impression."
            
            dpo_record = {
                "conversations": [
                    {"role": "user", "content": [
                        {"type": "image", "image": full_img_path},
                        {"type": "text", "text": user_prompt}
                    ]},
                    {"role": "assistant", "content": [
                        {"type": "text", "text": original_report}
                    ]}
                ],
                "rejected": [
                    {"role": "assistant", "content": [
                        {"type": "text", "text": negative_report}
                    ]}
                ],
                "reject_level": intensity
            }
            
            dpo_records.append(dpo_record)
            
        except Exception as e:
            print(f"Error processing sample {idx}: {e}")
            failed_count += 1
            continue

    print(f"Successfully processed {len(dpo_records)} samples, {failed_count} failed")

    # ä¿å­˜DPOæ•°æ®é›†
    print(f"Saving DPO dataset to {OUTPUT_JSONL}...")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(OUTPUT_JSONL), exist_ok=True)
    
    with open(OUTPUT_JSONL, "w") as f:
        for record in dpo_records:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")

    print(f"âœ… DPO dataset saved! Total samples: {len(dpo_records)}")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_processed = len(dpo_records) + failed_count
    success_rate = len(dpo_records) / total_processed if total_processed > 0 else 0
    gpt4o_rate = gpt4o_count / len(dpo_records) if len(dpo_records) > 0 else 0
    simple_rate = simple_count / len(dpo_records) if len(dpo_records) > 0 else 0
    
    # ä¿å­˜è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "total_samples": len(dpo_records),
        "failed_samples": failed_count,
        "success_rate": success_rate,
        "generation_methods": {
            "gpt4o_count": gpt4o_count,
            "simple_count": simple_count,
            "gpt4o_rate": gpt4o_rate,
            "simple_rate": simple_rate
        },
        "intensity_distribution": {
            "light": intensity_stats["light"],
            "medium": intensity_stats["medium"],
            "strong": intensity_stats["strong"],
            "light_rate": intensity_stats["light"] / len(dpo_records) if len(dpo_records) > 0 else 0,
            "medium_rate": intensity_stats["medium"] / len(dpo_records) if len(dpo_records) > 0 else 0,
            "strong_rate": intensity_stats["strong"] / len(dpo_records) if len(dpo_records) > 0 else 0
        },
        "max_samples_processed": MAX_SAMPLES,
        "api_key_set": bool(API_KEY)
    }
    
    stats_file = OUTPUT_JSONL.replace('.jsonl', '_stats.json')
    with open(stats_file, 'w') as f:
        json.dump(stats, f, indent=2)
    
    print("\nğŸ“Š Dataset Statistics:")
    print("=" * 50)
    print(f"  Total samples processed: {len(dpo_records)}")
    print(f"  Failed samples: {failed_count}")
    print(f"  Success rate: {success_rate:.2%}")
    print("\nğŸ¤– Negative Sample Generation:")
    print(f"  GPT-4o generated: {gpt4o_count} ({gpt4o_rate:.2%})")
    print(f"  Simple rule-based: {simple_count} ({simple_rate:.2%})")
    print("\nğŸ“ˆ Intensity Distribution:")
    print(f"  Light interference: {intensity_stats['light']} ({intensity_stats['light']/len(dpo_records)*100:.1f}%)")
    print(f"  Medium interference: {intensity_stats['medium']} ({intensity_stats['medium']/len(dpo_records)*100:.1f}%)")
    print(f"  Strong interference: {intensity_stats['strong']} ({intensity_stats['strong']/len(dpo_records)*100:.1f}%)")
    print(f"\nğŸ“ Files saved:")
    print(f"  Dataset: {OUTPUT_JSONL}")
    print(f"  Statistics: {stats_file}")
    
    # æ˜¾ç¤ºGPT-4oè´¹ç”¨ä¼°ç®—
    if gpt4o_count > 0:
        # GPT-4oå®šä»· (2024å¹´)
        input_cost_per_1m_tokens = 2.50  # ç¾å…ƒ
        output_cost_per_1m_tokens = 10.00  # ç¾å…ƒ
        
        # ä¼°ç®—æ¯ä¸ªæ ·æœ¬çš„tokenæ•°é‡
        avg_input_tokens_per_sample = 800   # åŒ…å«system prompt, user promptå’ŒåŸå§‹æŠ¥å‘Š
        avg_output_tokens_per_sample = 400  # ç”Ÿæˆçš„è´Ÿæ ·æœ¬æŠ¥å‘Š
        
        # è®¡ç®—æ€»è´¹ç”¨
        total_input_tokens = gpt4o_count * avg_input_tokens_per_sample
        total_output_tokens = gpt4o_count * avg_output_tokens_per_sample
        
        input_cost = (total_input_tokens / 1_000_000) * input_cost_per_1m_tokens
        output_cost = (total_output_tokens / 1_000_000) * output_cost_per_1m_tokens
        total_cost = input_cost + output_cost
        
        # è®¡ç®—å¹³å‡æ ·æœ¬è´¹ç”¨
        avg_cost_per_sample = total_cost / gpt4o_count if gpt4o_count > 0 else 0
        
        print(f"\nğŸ’° GPT-4o API Cost Estimation:")
        print(f"  Samples processed with GPT-4o: {gpt4o_count}")
        print(f"  Estimated input tokens: {total_input_tokens:,}")
        print(f"  Estimated output tokens: {total_output_tokens:,}")
        print(f"  Input cost: ${input_cost:.4f}")
        print(f"  Output cost: ${output_cost:.4f}")
        print(f"  Average cost per sample: ${avg_cost_per_sample:.4f}")
        print(f"  Total estimated cost: ${total_cost:.2f}")
        
        # ä¿å­˜è´¹ç”¨ä¿¡æ¯åˆ°ç»Ÿè®¡æ–‡ä»¶
        stats["cost_estimation"] = {
            "gpt4o_samples": gpt4o_count,
            "avg_input_tokens_per_sample": avg_input_tokens_per_sample,
            "avg_output_tokens_per_sample": avg_output_tokens_per_sample,
            "total_input_tokens": total_input_tokens,
            "total_output_tokens": total_output_tokens,
            "input_cost": input_cost,
            "output_cost": output_cost,
            "avg_cost_per_sample": avg_cost_per_sample,
            "total_cost": total_cost,
            "pricing_info": {
                "input_cost_per_1m_tokens": input_cost_per_1m_tokens,
                "output_cost_per_1m_tokens": output_cost_per_1m_tokens,
                "currency": "USD",
                "date": "2024"
            }
        }
        
        # é‡æ–°ä¿å­˜åŒ…å«è´¹ç”¨ä¿¡æ¯çš„ç»Ÿè®¡æ–‡ä»¶
        with open(stats_file, 'w') as f:
            json.dump(stats, f, indent=2)
    
    if simple_count > 0:
        print(f"\nâš ï¸  {simple_count} samples used fallback generation due to API issues")

if __name__ == "__main__":
    
    build_dpo_dataset()
