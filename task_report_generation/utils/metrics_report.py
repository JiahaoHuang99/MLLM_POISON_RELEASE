# ============================================================
# qwen/utils/metrics_report.py
# å¢å¼ºç‰ˆè¯„ä¼°æŒ‡æ ‡æ¨¡å—ï¼šé€‚ç”¨äºæŠ¥å‘Šå’Œæ·±åº¦åˆ†æçš„VQA-RADè¯„ä¼°æŒ‡æ ‡
# ============================================================

import numpy as np
import pandas as pd
from collections import defaultdict, Counter
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction, corpus_bleu
from rouge_score import rouge_scorer
from typing import Dict, List, Tuple, Optional
import json
import re
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
# import matplotlib.pyplot as plt
# import seaborn as sns


def normalize_text(s: str) -> str:
    """
    æ–‡æœ¬å½’ä¸€åŒ–ï¼šå°å†™ + å»æ‰æ ‡ç‚¹ + å»ç©ºæ ¼
    """
    if not isinstance(s, str):
        s = str(s)
    s = s.lower().strip()
    for ch in [".", ",", "!", "?", ":", ";"]:
        s = s.replace(ch, "")
    return s


def categorize_question_type(question: str) -> str:
    """
    æ ¹æ®é—®é¢˜ç±»å‹åˆ†ç±»
    """
    question = question.lower()
    if any(word in question for word in ['what', 'which', 'where', 'when', 'how']):
        return 'open_ended'
    elif any(word in question for word in ['is', 'are', 'does', 'do', 'can', 'will']):
        return 'yes_no'
    elif any(word in question for word in ['how many', 'how much', 'count']):
        return 'counting'
    else:
        return 'other'


def calculate_confidence_metrics(scores: List[float]) -> Dict[str, float]:
    """
    è®¡ç®—ç½®ä¿¡åº¦ç›¸å…³æŒ‡æ ‡
    """
    if not scores:
        return {}
    
    scores_array = np.array(scores)
    return {
        'mean_score': float(np.mean(scores_array)),
        'std_score': float(np.std(scores_array)),
        'min_score': float(np.min(scores_array)),
        'max_score': float(np.max(scores_array)),
        'median_score': float(np.median(scores_array)),
        'q25_score': float(np.percentile(scores_array, 25)),
        'q75_score': float(np.percentile(scores_array, 75))
    }


def evaluate_vqa_metrics(results):
    """
    åŸºç¡€VQAè¯„ä¼°æŒ‡æ ‡ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    """
    smooth_fn = SmoothingFunction().method1
    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)

    total_yesno = 0
    correct_yesno = 0
    substring_correct = 0
    bleu_scores = []
    rouge_scores = []

    for r in results:
        gt = normalize_text(r.get("gt_answer", ""))
        pred = normalize_text(r.get("pred_answer", ""))

        # Yes/No Accuracy
        if gt in ["yes", "no"]:
            total_yesno += 1
            if gt == pred or pred.startswith(gt):
                correct_yesno += 1

        # Substring Match
        if gt and gt in pred:
            substring_correct += 1
            bleu = 1.0
            rouge_l = 1.0
        else:
            bleu = sentence_bleu([gt.split()], pred.split(), smoothing_function=smooth_fn)
            rouge_l = rouge.score(gt, pred)["rougeL"].fmeasure

        bleu_scores.append(bleu)
        rouge_scores.append(rouge_l)

    accuracy = correct_yesno / total_yesno if total_yesno > 0 else 0.0
    substring_acc = substring_correct / len(results) if len(results) > 0 else 0.0
    bleu_avg = sum(bleu_scores) / len(bleu_scores)
    rouge_avg = sum(rouge_scores) / len(rouge_scores)

    metrics = {
        "accuracy_yesno": accuracy,
        "substring_acc": substring_acc,
        "bleu": bleu_avg,
        "rougeL": rouge_avg
    }

    return metrics


def evaluate_report_generation_metrics(results: List[Dict]) -> Dict:
    """
    ä¸“é—¨ç”¨äºæŠ¥å‘Šç”Ÿæˆçš„è¯„ä¼°æŒ‡æ ‡
    åŒ…å«åŒ»å­¦æŠ¥å‘Šç”Ÿæˆä¸­å¸¸ç”¨çš„æŒ‡æ ‡
    """
    smooth_fn = SmoothingFunction().method1
    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    
    # å‡†å¤‡æ•°æ®
    references = []
    predictions = []
    bleu_scores = []
    rouge1_scores = []
    rouge2_scores = []
    rougeL_scores = []
    
    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŒ‡æ ‡
    for r in results:
        gt = r.get("gt_answer", "").strip()
        pred = r.get("pred_answer", "").strip()
        
        if not gt or not pred:
            continue
            
        references.append(gt)
        predictions.append(pred)
        
        # BLEU score
        bleu = sentence_bleu([gt.split()], pred.split(), smoothing_function=smooth_fn)
        bleu_scores.append(bleu)
        
        # ROUGE scores
        rouge_scores = rouge.score(gt, pred)
        rouge1_scores.append(rouge_scores["rouge1"].fmeasure)
        rouge2_scores.append(rouge_scores["rouge2"].fmeasure)
        rougeL_scores.append(rouge_scores["rougeL"].fmeasure)
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0.0
    avg_rouge1 = np.mean(rouge1_scores) if rouge1_scores else 0.0
    avg_rouge2 = np.mean(rouge2_scores) if rouge2_scores else 0.0
    avg_rougeL = np.mean(rougeL_scores) if rougeL_scores else 0.0
    
    # è®¡ç®—corpus-level BLEU
    if references and predictions:
        # å‡†å¤‡corpus BLEUæ ¼å¼
        refs_corpus = [[ref.split()] for ref in references]
        preds_corpus = [pred.split() for pred in predictions]
        corpus_bleu_score = corpus_bleu(refs_corpus, preds_corpus, smoothing_function=smooth_fn)
    else:
        corpus_bleu_score = 0.0
    
    # è®¡ç®—ç²¾ç¡®åŒ¹é…å’Œå­ä¸²åŒ¹é…
    exact_match = sum(1 for gt, pred in zip(references, predictions) if gt == pred)
    exact_match_acc = exact_match / len(references) if references else 0.0
    
    substring_match = sum(1 for gt, pred in zip(references, predictions) if gt in pred or pred in gt)
    substring_acc = substring_match / len(references) if references else 0.0
    
    # è®¡ç®—é•¿åº¦ç»Ÿè®¡
    gt_lengths = [len(gt.split()) for gt in references]
    pred_lengths = [len(pred.split()) for pred in predictions]
    
    length_metrics = {
        'avg_gt_length': np.mean(gt_lengths) if gt_lengths else 0.0,
        'avg_pred_length': np.mean(pred_lengths) if pred_lengths else 0.0,
        'length_ratio': np.mean(pred_lengths) / np.mean(gt_lengths) if gt_lengths and np.mean(gt_lengths) > 0 else 0.0
    }
    
    # è®¡ç®—åŒ»å­¦ç›¸å…³æŒ‡æ ‡
    medical_metrics = calculate_medical_metrics(references, predictions)
    
    return {
        'bleu_1': avg_bleu,
        'bleu_corpus': corpus_bleu_score,
        'rouge_1': avg_rouge1,
        'rouge_2': avg_rouge2,
        'rouge_l': avg_rougeL,
        'exact_match': exact_match_acc,
        'substring_match': substring_acc,
        'length_metrics': length_metrics,
        'medical_metrics': medical_metrics,
        'num_samples': len(references)
    }


def calculate_medical_metrics(references: List[str], predictions: List[str]) -> Dict:
    """
    è®¡ç®—åŒ»å­¦æŠ¥å‘Šç›¸å…³çš„ç‰¹å®šæŒ‡æ ‡
    """
    # åŒ»å­¦å…³é”®è¯
    medical_keywords = [
        'normal', 'abnormal', 'clear', 'opacity', 'consolidation', 'effusion',
        'pneumonia', 'atelectasis', 'cardiomegaly', 'pneumothorax', 'edema',
        'infiltrate', 'mass', 'nodule', 'fracture', 'displacement'
    ]
    
    # è®¡ç®—å…³é”®è¯è¦†ç›–ç‡
    gt_keywords = set()
    pred_keywords = set()
    
    for ref, pred in zip(references, predictions):
        ref_lower = ref.lower()
        pred_lower = pred.lower()
        
        for keyword in medical_keywords:
            if keyword in ref_lower:
                gt_keywords.add(keyword)
            if keyword in pred_lower:
                pred_keywords.add(keyword)
    
    # å…³é”®è¯ç²¾ç¡®åº¦å’Œå¬å›ç‡
    if gt_keywords:
        keyword_precision = len(gt_keywords.intersection(pred_keywords)) / len(pred_keywords) if pred_keywords else 0.0
        keyword_recall = len(gt_keywords.intersection(pred_keywords)) / len(gt_keywords)
        keyword_f1 = 2 * keyword_precision * keyword_recall / (keyword_precision + keyword_recall) if (keyword_precision + keyword_recall) > 0 else 0.0
    else:
        keyword_precision = keyword_recall = keyword_f1 = 0.0
    
    # è®¡ç®—å¥å­ç»“æ„æŒ‡æ ‡
    sentence_metrics = calculate_sentence_metrics(references, predictions)
    
    return {
        'keyword_precision': keyword_precision,
        'keyword_recall': keyword_recall,
        'keyword_f1': keyword_f1,
        'unique_gt_keywords': len(gt_keywords),
        'unique_pred_keywords': len(pred_keywords),
        'sentence_metrics': sentence_metrics
    }


def calculate_sentence_metrics(references: List[str], predictions: List[str]) -> Dict:
    """
    è®¡ç®—å¥å­ç»“æ„ç›¸å…³æŒ‡æ ‡
    """
    gt_sentences = []
    pred_sentences = []
    
    for ref, pred in zip(references, predictions):
        # ç®€å•çš„å¥å­åˆ†å‰²ï¼ˆæŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·ï¼‰
        gt_sents = re.split(r'[.!?]+', ref)
        pred_sents = re.split(r'[.!?]+', pred)
        
        gt_sentences.extend([s.strip() for s in gt_sents if s.strip()])
        pred_sentences.extend([s.strip() for s in pred_sents if s.strip()])
    
    # è®¡ç®—å¹³å‡å¥å­é•¿åº¦
    gt_sent_lengths = [len(s.split()) for s in gt_sentences if s]
    pred_sent_lengths = [len(s.split()) for s in pred_sentences if s]
    
    return {
        'avg_gt_sentence_length': np.mean(gt_sent_lengths) if gt_sent_lengths else 0.0,
        'avg_pred_sentence_length': np.mean(pred_sent_lengths) if pred_sent_lengths else 0.0,
        'gt_sentence_count': len(gt_sentences),
        'pred_sentence_count': len(pred_sentences)
    }


def evaluate_comprehensive_metrics(results: List[Dict], 
                                 include_error_analysis: bool = True,
                                 include_category_analysis: bool = True) -> Dict:
    """
    å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡åˆ†æ
    
    Args:
        results: List[Dict]ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
            {
                "gt_answer": str,
                "pred_answer": str,
                "question": str (å¯é€‰),
                "confidence": float (å¯é€‰),
                "category": str (å¯é€‰)
            }
        include_error_analysis: æ˜¯å¦åŒ…å«é”™è¯¯åˆ†æ
        include_category_analysis: æ˜¯å¦åŒ…å«åˆ†ç±»åˆ†æ
    
    Returns:
        comprehensive_metrics: Dict åŒ…å«è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡
    """
    smooth_fn = SmoothingFunction().method1
    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    
    # åŸºç¡€æŒ‡æ ‡
    total_samples = len(results)
    total_yesno = 0
    correct_yesno = 0
    substring_correct = 0
    exact_match_correct = 0
    
    bleu_scores = []
    rouge_scores = []
    confidences = []
    
    # åˆ†ç±»ç»Ÿè®¡
    category_stats = defaultdict(lambda: {'total': 0, 'correct': 0, 'bleu_scores': [], 'rouge_scores': []})
    
    # é”™è¯¯åˆ†æ
    error_cases = []
    failure_patterns = defaultdict(int)
    
    for idx, r in enumerate(results):
        gt = normalize_text(r.get("gt_answer", ""))
        pred = normalize_text(r.get("pred_answer", ""))
        question = r.get("question", "")
        confidence = r.get("confidence", None)
        category = r.get("category", "")
        
        # è‡ªåŠ¨åˆ†ç±»é—®é¢˜ç±»å‹ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼‰
        if not category and question:
            category = categorize_question_type(question)
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        if confidence is not None:
            confidences.append(confidence)
        
        # 1ï¸âƒ£ Yes/No Accuracy
        is_yesno = gt in ["yes", "no"]
        if is_yesno:
            total_yesno += 1
            if gt == pred or pred.startswith(gt):
                correct_yesno += 1
        
        # 2ï¸âƒ£ ç²¾ç¡®åŒ¹é…
        if gt == pred:
            exact_match_correct += 1
        
        # 3ï¸âƒ£ å­ä¸²åŒ¹é…
        if gt and gt in pred:
            substring_correct += 1
            bleu = 1.0
            rouge_l = 1.0
        else:
            bleu = sentence_bleu([gt.split()], pred.split(), smoothing_function=smooth_fn)
            rouge_l = rouge.score(gt, pred)["rougeL"].fmeasure
            
            # é”™è¯¯åˆ†æ
            if include_error_analysis and bleu < 0.3:  # ä½BLEUåˆ†æ•°è®¤ä¸ºæ˜¯é”™è¯¯
                error_cases.append({
                    'index': idx,
                    'question': question,
                    'gt_answer': r.get("gt_answer", ""),
                    'pred_answer': r.get("pred_answer", ""),
                    'bleu_score': bleu,
                    'rouge_score': rouge_l,
                    'category': category
                })
                
                # é”™è¯¯æ¨¡å¼åˆ†æ
                if is_yesno:
                    failure_patterns[f"yesno_wrong_{gt}"] += 1
                else:
                    failure_patterns[f"open_ended_low_bleu"] += 1
        
        bleu_scores.append(bleu)
        rouge_scores.append(rouge_l)
        
        # åˆ†ç±»ç»Ÿè®¡
        if category:
            category_stats[category]['total'] += 1
            category_stats[category]['bleu_scores'].append(bleu)
            category_stats[category]['rouge_scores'].append(rouge_l)
            if gt == pred or (gt and gt in pred):
                category_stats[category]['correct'] += 1
    
    # è®¡ç®—åŸºç¡€æŒ‡æ ‡
    accuracy_yesno = correct_yesno / total_yesno if total_yesno > 0 else 0.0
    exact_match_acc = exact_match_correct / total_samples
    substring_acc = substring_correct / total_samples
    bleu_avg = np.mean(bleu_scores)
    rouge_avg = np.mean(rouge_scores)
    
    # ç½®ä¿¡åº¦åˆ†æ
    confidence_metrics = calculate_confidence_metrics(confidences) if confidences else {}
    
    # åˆ†ç±»åˆ†æ
    category_analysis = {}
    if include_category_analysis:
        for cat, stats in category_stats.items():
            if stats['total'] > 0:
                category_analysis[cat] = {
                    'total_samples': stats['total'],
                    'accuracy': stats['correct'] / stats['total'],
                    'avg_bleu': np.mean(stats['bleu_scores']),
                    'avg_rouge': np.mean(stats['rouge_scores']),
                    'bleu_std': np.std(stats['bleu_scores']),
                    'rouge_std': np.std(stats['rouge_scores'])
                }
    
    # æ„å»ºç»¼åˆæŒ‡æ ‡
    comprehensive_metrics = {
        # åŸºç¡€æŒ‡æ ‡
        'basic_metrics': {
            'total_samples': total_samples,
            'yesno_accuracy': accuracy_yesno,
            'exact_match_accuracy': exact_match_acc,
            'substring_accuracy': substring_acc,
            'avg_bleu': bleu_avg,
            'avg_rouge': rouge_avg,
            'bleu_std': np.std(bleu_scores),
            'rouge_std': np.std(rouge_scores)
        },
        
        # ç½®ä¿¡åº¦åˆ†æ
        'confidence_analysis': confidence_metrics,
        
        # åˆ†ç±»åˆ†æ
        'category_analysis': category_analysis,
        
        # é”™è¯¯åˆ†æ
        'error_analysis': {
            'total_errors': len(error_cases),
            'error_rate': len(error_cases) / total_samples,
            'failure_patterns': dict(failure_patterns),
            'worst_cases': error_cases[:10] if include_error_analysis else []  # å‰10ä¸ªæœ€å·®æ¡ˆä¾‹
        },
        
        # è¯¦ç»†åˆ†æ•°åˆ†å¸ƒ
        'score_distribution': {
            'bleu_scores': bleu_scores,
            'rouge_scores': rouge_scores,
            'bleu_histogram': np.histogram(bleu_scores, bins=10, range=(0, 1))[0].tolist(),
            'rouge_histogram': np.histogram(rouge_scores, bins=10, range=(0, 1))[0].tolist()
        }
    }
    
    return comprehensive_metrics


# def generate_report_summary(metrics: Dict, save_path: Optional[str] = None) -> str:
#     """
#     ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šæ‘˜è¦
    
#     Args:
#         metrics: æ¥è‡ª evaluate_comprehensive_metrics çš„ç»“æœ
#         save_path: å¯é€‰çš„ä¿å­˜è·¯å¾„
    
#     Returns:
#         report_summary: æ ¼å¼åŒ–çš„æŠ¥å‘Šå­—ç¬¦ä¸²
#     """
#     basic = metrics['basic_metrics']
#     confidence = metrics['confidence_analysis']
#     category = metrics['category_analysis']
#     error = metrics['error_analysis']
    
#     report = f"""
# # VQA-RAD è¯„ä¼°æŠ¥å‘Š

# ## ğŸ“Š åŸºç¡€æŒ‡æ ‡æ¦‚è§ˆ
# - **æ€»æ ·æœ¬æ•°**: {basic['total_samples']}
# - **Yes/No å‡†ç¡®ç‡**: {basic['yesno_accuracy']:.3f}
# - **ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡**: {basic['exact_match_accuracy']:.3f}
# - **å­ä¸²åŒ¹é…å‡†ç¡®ç‡**: {basic['substring_accuracy']:.3f}
# - **å¹³å‡ BLEU åˆ†æ•°**: {basic['avg_bleu']:.3f} Â± {basic['bleu_std']:.3f}
# - **å¹³å‡ ROUGE-L åˆ†æ•°**: {basic['avg_rouge']:.3f} Â± {basic['rouge_std']:.3f}

# ## ğŸ“ˆ ç½®ä¿¡åº¦åˆ†æ
# """
    
#     if confidence:
#         report += f"""
# - **å¹³å‡ç½®ä¿¡åº¦**: {confidence['mean_score']:.3f}
# - **ç½®ä¿¡åº¦æ ‡å‡†å·®**: {confidence['std_score']:.3f}
# - **ç½®ä¿¡åº¦èŒƒå›´**: [{confidence['min_score']:.3f}, {confidence['max_score']:.3f}]
# - **ä¸­ä½æ•°ç½®ä¿¡åº¦**: {confidence['median_score']:.3f}
# """
#     else:
#         report += "- æ— ç½®ä¿¡åº¦æ•°æ®\n"
    
#     report += f"""
# ## ğŸ·ï¸ åˆ†ç±»åˆ«åˆ†æ
# """
    
#     if category:
#         for cat, stats in category.items():
#             report += f"""
# ### {cat.title()}
# - **æ ·æœ¬æ•°**: {stats['total_samples']}
# - **å‡†ç¡®ç‡**: {stats['accuracy']:.3f}
# - **å¹³å‡ BLEU**: {stats['avg_bleu']:.3f} Â± {stats['bleu_std']:.3f}
# - **å¹³å‡ ROUGE**: {stats['avg_rouge']:.3f} Â± {stats['rouge_std']:.3f}
# """
#     else:
#         report += "- æ— åˆ†ç±»æ•°æ®\n"
    
#     report += f"""
# ## âŒ é”™è¯¯åˆ†æ
# - **é”™è¯¯æ ·æœ¬æ•°**: {error['total_errors']}
# - **é”™è¯¯ç‡**: {error['error_rate']:.3f}
# - **ä¸»è¦å¤±è´¥æ¨¡å¼**: {list(error['failure_patterns'].keys())}

# ## ğŸ“‹ æœ€å·®æ¡ˆä¾‹åˆ†æ (å‰5ä¸ª)
# """
    
#     for i, case in enumerate(error['worst_cases'][:5]):
#         report += f"""
# ### æ¡ˆä¾‹ {i+1}
# - **é—®é¢˜**: {case.get('question', 'N/A')}
# - **æ ‡å‡†ç­”æ¡ˆ**: {case.get('gt_answer', 'N/A')}
# - **é¢„æµ‹ç­”æ¡ˆ**: {case.get('pred_answer', 'N/A')}
# - **BLEU åˆ†æ•°**: {case.get('bleu_score', 0):.3f}
# - **ROUGE åˆ†æ•°**: {case.get('rouge_score', 0):.3f}
# - **ç±»åˆ«**: {case.get('category', 'N/A')}
# """
    
#     if save_path:
#         with open(save_path, 'w', encoding='utf-8') as f:
#             f.write(report)
#         print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")
    
#     return report


# def plot_metrics_distribution(metrics: Dict, save_path: Optional[str] = None):
#     """
#     ç»˜åˆ¶æŒ‡æ ‡åˆ†å¸ƒå›¾
    
#     Args:
#         metrics: æ¥è‡ª evaluate_comprehensive_metrics çš„ç»“æœ
#         save_path: å¯é€‰çš„ä¿å­˜è·¯å¾„
#     """
#     fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
#     # BLEU åˆ†æ•°åˆ†å¸ƒ
#     axes[0, 0].hist(metrics['score_distribution']['bleu_scores'], bins=20, alpha=0.7, color='blue')
#     axes[0, 0].set_title('BLEU Score Distribution')
#     axes[0, 0].set_xlabel('BLEU Score')
#     axes[0, 0].set_ylabel('Frequency')
    
#     # ROUGE åˆ†æ•°åˆ†å¸ƒ
#     axes[0, 1].hist(metrics['score_distribution']['rouge_scores'], bins=20, alpha=0.7, color='green')
#     axes[0, 1].set_title('ROUGE-L Score Distribution')
#     axes[0, 1].set_xlabel('ROUGE-L Score')
#     axes[0, 1].set_ylabel('Frequency')
    
#     # åˆ†ç±»åˆ«å‡†ç¡®ç‡
#     if metrics['category_analysis']:
#         categories = list(metrics['category_analysis'].keys())
#         accuracies = [metrics['category_analysis'][cat]['accuracy'] for cat in categories]
        
#         axes[1, 0].bar(categories, accuracies, alpha=0.7, color='orange')
#         axes[1, 0].set_title('Accuracy by Category')
#         axes[1, 0].set_ylabel('Accuracy')
#         axes[1, 0].tick_params(axis='x', rotation=45)
    
#     # å¤±è´¥æ¨¡å¼åˆ†æ
#     if metrics['error_analysis']['failure_patterns']:
#         patterns = list(metrics['error_analysis']['failure_patterns'].keys())
#         counts = list(metrics['error_analysis']['failure_patterns'].values())
        
#         axes[1, 1].bar(patterns, counts, alpha=0.7, color='red')
#         axes[1, 1].set_title('Failure Patterns')
#         axes[1, 1].set_ylabel('Count')
#         axes[1, 1].tick_params(axis='x', rotation=45)
    
#     plt.tight_layout()
    
#     if save_path:
#         plt.savefig(save_path, dpi=300, bbox_inches='tight')
#         print(f"å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
    
#     plt.show()


# def export_metrics_to_json(metrics: Dict, save_path: str):
#     """
#     å°†æŒ‡æ ‡å¯¼å‡ºä¸ºJSONæ ¼å¼
    
#     Args:
#         metrics: æ¥è‡ª evaluate_comprehensive_metrics çš„ç»“æœ
#         save_path: ä¿å­˜è·¯å¾„
#     """
#     with open(save_path, 'w', encoding='utf-8') as f:
#         json.dump(metrics, f, ensure_ascii=False, indent=2)
#     print(f"æŒ‡æ ‡å·²å¯¼å‡ºåˆ°: {save_path}")


# # ä¾¿æ·å‡½æ•°ï¼šä¸€é”®ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
# def generate_full_report(results: List[Dict], 
#                         output_dir: str = "./report_output",
#                         model_name: str = "Unknown Model") -> Dict:
#     """
#     ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š
    
#     Args:
#         results: è¯„ä¼°ç»“æœåˆ—è¡¨
#         output_dir: è¾“å‡ºç›®å½•
#         model_name: æ¨¡å‹åç§°
    
#     Returns:
#         comprehensive_metrics: å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡
#     """
#     import os
    
#     # åˆ›å»ºè¾“å‡ºç›®å½•
#     os.makedirs(output_dir, exist_ok=True)
    
#     # è®¡ç®—ç»¼åˆæŒ‡æ ‡
#     metrics = evaluate_comprehensive_metrics(results, 
#                                            include_error_analysis=True,
#                                            include_category_analysis=True)
    
#     # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
#     report_path = os.path.join(output_dir, f"{model_name}_evaluation_report.md")
#     generate_report_summary(metrics, report_path)
    
#     # ç”Ÿæˆå›¾è¡¨
#     plot_path = os.path.join(output_dir, f"{model_name}_metrics_distribution.png")
#     plot_metrics_distribution(metrics, plot_path)
    
#     # å¯¼å‡ºJSON
#     json_path = os.path.join(output_dir, f"{model_name}_metrics.json")
#     export_metrics_to_json(metrics, json_path)
    
#     print(f"\nğŸ‰ å®Œæ•´æŠ¥å‘Šå·²ç”Ÿæˆï¼")
#     print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
#     print(f"ğŸ“„ æ–‡æœ¬æŠ¥å‘Š: {report_path}")
#     print(f"ğŸ“Š åˆ†å¸ƒå›¾è¡¨: {plot_path}")
#     print(f"ğŸ“‹ JSONæ•°æ®: {json_path}")
    
#     return metrics
