#!/usr/bin/env python3
"""
å°† SFT åçš„ LoRA æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­
ç”±äº EasyR1 ç›®å‰ä¸æ”¯æŒç›´æ¥åŠ è½½ LoRA æƒé‡ï¼Œéœ€è¦å…ˆåˆå¹¶åˆ°åŸºç¡€æ¨¡å‹
"""

import argparse
import os
import torch
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
from peft import PeftModel


def merge_lora_to_base_model(
    base_model_path: str,
    lora_model_path: str,
    output_path: str,
    cache_dir: str = None
):
    """
    å°† LoRA æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹å¹¶ä¿å­˜
    
    Args:
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆHugging Face Hub ID æˆ–æœ¬åœ°è·¯å¾„ï¼‰
        lora_model_path: LoRA æƒé‡è·¯å¾„
        output_path: åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„
        cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
    """
    print(f"ğŸ“¥ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
    base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        cache_dir=cache_dir,
    )
    
    print(f"ğŸ“¥ åŠ è½½ LoRA æƒé‡: {lora_model_path}")
    model = PeftModel.from_pretrained(base_model, lora_model_path)
    
    print("ğŸ”„ åˆå¹¶æƒé‡...")
    merged_model = model.merge_and_unload()
    
    print(f"ğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {output_path}")
    os.makedirs(output_path, exist_ok=True)
    merged_model.save_pretrained(
        output_path,
        safe_serialization=True,
        max_shard_size="5GB"
    )
    
    # åŒæ—¶ä¿å­˜ processor
    print("ğŸ’¾ ä¿å­˜ processor...")
    processor = AutoProcessor.from_pretrained(
        base_model_path,
        cache_dir=cache_dir
    )
    processor.save_pretrained(output_path)
    
    print("âœ… åˆå¹¶å®Œæˆï¼")
    print(f"   åˆå¹¶åçš„æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
    print(f"   è¯·åœ¨ config.yaml ä¸­å°† worker.actor.model.model_path è®¾ç½®ä¸º: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹")
    parser.add_argument("--base_model", type=str, 
                       default="Qwen/Qwen2.5-VL-3B-Instruct",
                       help="åŸºç¡€æ¨¡å‹è·¯å¾„æˆ– Hugging Face Hub ID")
    parser.add_argument("--lora_model", type=str, required=True,
                       help="LoRA æƒé‡è·¯å¾„")
    parser.add_argument("--output_path", type=str, required=True,
                       help="åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„")
    parser.add_argument("--cache_dir", type=str, default=None,
                       help="æ¨¡å‹ç¼“å­˜ç›®å½•")
    
    args = parser.parse_args()
    
    merge_lora_to_base_model(
        base_model_path=args.base_model,
        lora_model_path=args.lora_model,
        output_path=args.output_path,
        cache_dir=args.cache_dir
    )


if __name__ == "__main__":
    main()

